---
title: "Lab 1"
output: html_document
---

## From Introduction to GLMs

:::{.question}

Formulate, by hand, the model used in the medical school admissions example as a GLM.

You should carefully identify:

+ The random component
+ The systematic component
+ The link function
+ Identify your named coefficients with data covariates

:::
<!-- end of Task -->

:::{.content-visible when-profile="answers"}
:::{.answer}

Random component: Let $Y_i=1$ if the $i$th applicant is accepted to medical school and $Y_i=0$ if not. We assume that the $Y_i$ are independent responses from $\text{Bin}(1,p_i)$, with $E(Y_i)=p_i$ for $i=1,\dots,55$.

Systematic component: $\beta_0+\beta_1 x_i$ where $x_i$ is the $i$th applicant's GPA and $\beta_0$ and $\beta_1$ are parameters to be estimated.

Link function: $g(p_i)=\log \left(\dfrac{p_i}{1-p_i} \right)$ (logit link)

Equation of the GLM: $$\log \left(\dfrac{p_i}{1-p_i} \right)=\beta_0+\beta_1 x_i.$$ 

Notation: $\beta_0$ will be the intercept and $\beta_1$ the coefficient on each student's GPA.

:::
:::
<!-- end of Answer -->

:::{.question}

Using this code (taken from the notes) to load the GPA dataset,

```{r}
library(Stat2Data)
library(ggplot2)
data(MedGPA)
head(MedGPA)
```

(a) Type the code to fit a logistic regression model to this data to predict `Acceptance` from just `GPA`, saving the result into a variable called `med.gpa`
(b) Predict the acceptance probability for an applicant with a GPA of
    (i) 2.5 
    (ii) 3 
    (iii) 4 

    First do this "by hand" using the regression equation, then in R using the `predict()` function. To use `predict()` a helpful shortcut can be to create a simple data.frame with three students in.

    *Hint*: The `predict()` function will return values on the linear predictor scale unless you use the `type` parameter.

(c) Select two or three further covariates from the data and fit a logistic regression with these additional covariates. Make some preliminary comments on what you discover.. (Don't get too detailed, the exercise is mostly about fitting the model, not analysis)

:::
<!-- end of question -->

:::{.content-visible when-profile="answers"}
:::{.answer}

(a) 
```{r}
med.glm <- glm(Acceptance ~ GPA, data = MedGPA, family = binomial)
```

(b)
In the *GPA and admission to medical school* example, we can write down the fitted model equation from the `summary(med.glm)`:

$$
\log\left(\frac{p_i}{1-p_i}\right) =-19.207 + 5.454 \times \text{GPA}_i
$$

From the fitted equation, we can obtain the acceptance probability by solving for $p_i$. The messy way is to go directly:

$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times \text{GPA}_i)} {1+ \exp(-19.207 + 5.454 \times \text{GPA}_i)} $$

Easier is to first work out the odds, then re-arrange that for $p_i$ with the same result.

To predict the acceptance probability for an applicant we just need to substitute that specified GPA score in the equation for $\hat{p}_i$:

(i) GPA = 2.5:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 2.5)} {1+ \exp(-19.207 + 5.454 \times 2.5)} \Rightarrow \hat{p}_i = 0.00378 $$

(ii) GPA = 3:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 3)} {1+ \exp(-19.207 + 5.454 \times 3)} \Rightarrow \hat{p}_i = 0.05494$$

(iii) GPA = 4:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 4)} {1+ \exp(-19.207 + 5.454 \times 4)} \Rightarrow \hat{p}_i = 0.93143$$


Alternatively, we can use the `predict()` function in `R`. You can manually call `predict` with each GPA value separately, or ... 

The most efficient way to use `predict()` is to create a new simple data.frame containing data for all three new students and then asking it to predict all the responses simultaneously (by passing `predict` the data.frame). Or you can use predict three times.

```{r}
#| label: r-medgpa-predict-values
predict(med.glm, data.frame(GPA = c(2.5, 3, 4)), type = 'response')
```

(c)
```{r}
med.glm.more <- glm(Acceptance ~ GPA + MCAT + Apps, data = MedGPA, family = binomial)
summary(med.glm.more)
```

I chose to add `MCAT` and `Apps`, and early evidence suggests that neither is as useful as `GPA` for model fit. Both do have positive coefficients, so presumably for these covariates having higher scores is *better*. However, a preliminary look at the standard errors (and p-values) I suspect there is not evidence to suggest either $\beta_2$ or $\beta_3$ are not zero.

:::
:::
<!-- end of Answer -->

## From Logistic Models

```{r}
#| label: r-load-yl53
yl<- read.csv('https://github.com/UofGAnalyticsData/APM/raw/refs/heads/main/yl53.csv')
head(yl)
yl$hear <- factor(yl$hear, levels = c("Laurel", "Yanny"))
```

:::{.question}



Fit appropriate logistic regression models to explore if gender is related to whether people hear "Yanny" or "Laurel".

:::
<!-- end of Task -->

:::{.content-visible when-profile="answers"}
:::{.answer}

We can fit a model with just `gender` as a predictor:

```{r}
#| label: r-yl-glm-gender-summary
mod.yl2 <- glm(hear ~ gender, family=binomial, data=yl)
summary(mod.yl2)

```

or we can add `gender` to the model with `age`:

```{r}
#| label: r-yl-glm-gender-age-summary
mod.yl3 <- glm(hear ~ gender+age, family=binomial, data=yl)
summary(mod.yl3)

```
In both cases we see that there is no significant gender effect.

:::
:::
<!-- end of Answer -->

**ADD QUESTIONS TO INTERPRET COEFFICIENTS FROM THE gender or age+gender model?**


### Case Study: The Challenger Disaster

*This case study is mostly tasks for you to get practice*

In January 1986, the [space shuttle Challenger exploded shortly after launch](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster). It was subsequently found that the rubber O-ring seals in the rocket boosters were susceptible to failing in low temperatures. At the time of the launch the temperature was 31 degrees Fahrenheit. Could the failure of the O-rings have been predicted?
Data from the previous 23 missions shows some evidence of damage on some of the $6$ O-rings on each shuttle, as well as the temperature during the shuttle launch. The data is available from `library(faraway)` and is called `orings`. The first column of the data gives the temperature at launch in degrees F and the second column gives the number of damage incidents out of $6$ possible.

Here are the first few rows of the data:

```{r}
library(faraway)
head(orings)
```

Predictor variable
: $x_i$ the temperature (in degrees F) during launch for the $i$th mission, $i=1,\dots,23$. 

Response variable
: $y_i$ is the number of damaged O-rings (out of 6 total).
  
Model setup
: the probability $p_i$ of individual damage to each O-ring means
$$
Y_i \overset{indep}\sim \text{Bin}(n,p_i)
$$ with $g(p_i)=\beta_0 + \beta_1 x_i$, and here $n=6$.

Here is a plot of the data:

```{r}
#| label: r-oring-data-plot
#| echo: true
p1<- ggplot(orings, aes(x=temp, y=damage/6)) + 
     geom_point()+ xlim (c(25,85)) + ylim(c(0,1)) + 
     xlab ("Temperature (F)") + ylab("Probability of damage")
```

```{r}
#| label: fig-r-oring-data-plot-show
#| fig-cap: "Scatterplot of the 18 datapoints, of temperature(F) against Proportion of rings damaged."
#| fig-alt: "Scatterplot of raw data for the oring data (18 points). Showing 80% failure at around 52 degrees F, then values around 16% or 0% for most values above 55 degrees. More zeros at higher temps."
#| echo: false
#| 
#p2 <- p1 + theme(panel.background = element_rect(fill = "transparent", colour = NA),
#              #plot.background = element_rect(fill = "transparent", colour = NA),
               #panel.border = element_rect(fill = NA, colour = "black"))
p1
```

:::{.question}



Fit a binomial regression model to the data, trying out the logit, probit and complementary log-log options for the link function. 

:::
<!-- end of Task -->

:::{.content-visible when-profile="answers"}
:::{.answer}

Logit link:

```{r, results= 'hide'}
lmod <- glm(cbind(damage, 6-damage) ~ temp, family=binomial, data=orings) 
summary(lmod)
```

Probit link:

```{r, results='hide'}

pmod <- glm(cbind(damage, 6-damage) ~ temp, family=binomial(link="probit"), 
            data=orings)
summary(pmod)
```

Complementary log-log link:

```{r, results= 'hide'}
cmod <- glm(cbind(damage, 6-damage) ~ temp, family=binomial(link="cloglog"), data=orings) 
summary(cmod)
```

:::
:::
<!-- end of Answer -->



:::{.question}



Superimpose the fitted probabilities from each of the three models on the above plot.

:::
<!-- end of Task -->

:::{.content-visible when-profile="answers"}
:::{.answer}

Here is some code for plotting the three fits.

```{r}
#| label: fig-r-oring-temp-prob-logits
#| fig-cap: "Fitted prediction curves for the three link functions, for the orings data."
#| fig-alt: "Temperature (x) against Probability of damage (y), raw data and predictions from our three link functions. Inverted S-shaped curves, from 1 down to 0 for temperatures from 30F to 80F. All similar, none fit the plotted raw data points very well."
#| fig-pos: "H" # Needed for echo: false to fix latex figure env bug

pred1 <- predict(lmod, newdata=data.frame(temp=seq(25,85,le=23)), type="response")
pred2 <- predict(pmod, newdata=data.frame(temp=seq(25,85,le=23)), type="response")
pred3 <- predict(cmod, newdata=data.frame(temp=seq(25,85,le=23)), type="response")
pred <- data.frame(logit = pred1, probit= pred2, cloglog=pred3, px = seq(25,85,le=23),orings)
p1.1 <- ggplot(pred, aes(x=orings$temp, y= orings$damage/6)) +  
        geom_point(size = 1)+ xlim (c(25,85)) + ylim(c(0,1)) + 
        xlab ("Temperature (F)") + ylab("Probability of damage") +
        geom_line(aes(x = px, y = logit, color = "Logit", linetype = "solid")) +
        geom_line(aes(x = px, y = probit, color = "Probit", linetype = "dashed"))+
        geom_line(aes(x = px, y = cloglog, color = "Complementary log-log", linetype = "dotted")) +
        guides(colour = guide_legend("Link function"), linetype = "none")
p1.1

```

:::
:::
<!-- end of Answer -->

:::{.question}



Calculate a point estimate of the probability of damage to the O-rings when the temperature is 31 degrees Fahrenheit using each of the three models. 

:::
<!-- end of Task -->

:::{.content-visible when-profile="answers"}
:::{.answer}

We can obtain the predicted probabilities using the model equation:

```{r}
exp(11.6630-0.2162*31)/(1+exp(11.6630-0.2162*31))
```

We can get the same answer using the `predict()` function as follows: 

```{r}
predict(lmod, newdata=data.frame(temp=31), type="response")
```

Similarly, we can obtain the prediction for the probit model using the cumulative distribution function of a normal distribution:

```{r}
pnorm(5.5915-0.1058*31) 
```

or by using the `predict()` function:

```{r}
predict(pmod, newdata=data.frame(temp=31), type="response")
```

Finally for the complementary log-log model the predicted probability is

```{r}
predict(cmod, newdata=data.frame(temp=31), type="response")
```

The predicted probability of damage is very high for all models.

:::
:::
<!-- end of Answer -->


:::{.question}

## Some calculation question

Get them to calculate the log odds ratio associated with some categorical variable.
Then same for odds ratio.
Then get them to try if $p_1=0.2$ and if $p_1=0.5$, in both cases find $p^{\text{new}}_1$ under a change in covariate.
* Do first for some binary covariate
* Then do for a 10-unit change in some cts covariate.

Calculate a confidence interval for an odds ratio above (maybe do the binary one).

:::

:::{.question}

## Do some deviance-based goodness of fit test

Fit binomial GLM wih `y ~ a + b`.
Compare with `y ~ a`.
With reference to deviances comment on results.
Compare AICs and comment too.
Limitations?

:::

:::{.question}

## Plot interpretation

Comment on this residuals plots from having fitted glm `y ~ a + b`.

:::