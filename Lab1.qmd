---
title: "Labby 1"
output: html_document
---

# Lab 1

Hello

## From IntroToGLMs

:::{.question}

## Question

Formulate, by hand, the model used in the medical school admissions example as a GLM.

:::
<!-- end of Task -->

::::{.answer}

## Answer

Random component: Let $Y_i=1$ if the $i$th applicant is accepted to medical school and $Y_i=0$ if not. We assume that the $Y_i$ are independent responses from $\text{Bin}(1,p_i)$, with $E(Y_i)=p_i$ for $i=1,\dots,55$.

Systematic component: $\beta_0+\beta_1 x_i$ where $x_i$ is the $i$th applicant's GPA and $\beta_0$ and $\beta_1$ are parameters to be estimated.

Link function: $g(p_i)=\log \left(\dfrac{p_i}{1-p_i} \right)$ (logit link)

Equation of the GLM: $$\log \left(\dfrac{p_i}{1-p_i} \right)=\beta_0+\beta_1 x_i.$$ 
::::
<!-- end of Answer -->

```{r}
library(Stat2Data)
library(ggplot2)
data(MedGPA)
med.glm <- glm(Acceptance ~ GPA, data = MedGPA, family = binomial)
```


:::{.question}

## Question

Predict the acceptance probability for an applicant with a GPA of (i) 2.5, (ii) 3 (iii) 4. First do this "by hand" using the regression equation, then in R using the `predict()` function.

*Hint: The* `predict()` *function will return values on the linear predictor scale unless you specify* `type='response'` *which returns probabilities instead.*
:::
<!-- end of Task -->

::::{.answer}

## Answer X

In the *GPA and admission to medical school* example, we can write down the fitted model equation from the `summary(med.glm)`:
$$\log\left(\frac{p_i}{1-p_i}\right) =-19.207 + 5.454 \times \text{GPA}$$

From the fitted equation, we can obtain the acceptance probability by solving for $p_i$:

$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times \text{GPA})} {1+ \exp(-19.207 + 5.454 \times \text{GPA})} $$

To predict the acceptance probability for an applicant we just need to substitute the specified GPA in the equation for $\hat{p}_i$:

(i) GPA = 2.5:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 2.5)} {1+ \exp(-19.207 + 5.454 \times 2.5)} \Rightarrow \hat{p}_i = 0.00378 $$

(ii) GPA = 3:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 3)} {1+ \exp(-19.207 + 5.454 \times 3)} \Rightarrow \hat{p}_i = 0.05494$$

(iii) GPA = 4:
$$\hat{p}_i = \frac{\exp(-19.207 + 5.454 \times 4)} {1+ \exp(-19.207 + 5.454 \times 4)} \Rightarrow \hat{p}_i = 0.93143$$


Alternatively, we can use the `predict()` function in `R` as follows:

```{r}
#| label: r-medgpa-predict-values
predict(med.glm, data.frame(GPA = c(2.5, 3, 4)), type = 'response')
```

::::
<!-- end of Answer -->



## From Logistic Models

```{r}
#| label: r-load-yl53
yl<- read.csv('https://github.com/UofGAnalyticsData/APM/raw/refs/heads/main/yl53.csv')
head(yl)
yl$hear <- factor(yl$hear, levels = c("Laurel", "Yanny"))
```

:::{.question}

## Question

Fit appropriate logistic regression models to explore if gender is related to whether people hear "Yanny" or "Laurel".

:::
<!-- end of Task -->


::::{.answer}

We can fit a model with just `gender` as a predictor:

```{r}
#| label: r-yl-glm-gender-summary
mod.yl2 <- glm(hear ~ gender, family=binomial, data=yl)
summary(mod.yl2)

```

or we can add `gender` to the model with `age`:

```{r}
#| label: r-yl-glm-gender-age-summary
mod.yl3 <- glm(hear ~ gender+age, family=binomial, data=yl)
summary(mod.yl3)

```
In both cases we see that there is no significant gender effect.

::::
<!-- end of Answer -->

**ADD QUESTIONS TO INTERPRET COEFFICIENTS FROM THE gender or age+gender model?**


### Case Study: The Challenger Disaster

*This case study is mostly tasks for you to get practice*

In January 1986, the [space shuttle Challenger exploded shortly after launch](https://en.wikipedia.org/wiki/Space_Shuttle_Challenger_disaster). It was subsequently found that the rubber O-ring seals in the rocket boosters were susceptible to failing in low temperatures. At the time of the launch the temperature was 31 degrees Fahrenheit. Could the failure of the O-rings have been predicted?
Data from the previous 23 missions shows some evidence of damage on some of the $6$ O-rings on each shuttle, as well as the temperature during the shuttle launch. The data is available from `library(faraway)` and is called `orings`. The first column of the data gives the temperature at launch in degrees F and the second column gives the number of damage incidents out of $6$ possible.

Here are the first few rows of the data:

```{r}
library(faraway)
head(orings)
```

Predictor variable
: $x_i$ the temperature (in degrees F) during launch for the $i$th mission, $i=1,\dots,23$. 

Response variable
: $y_i$ is the number of damaged O-rings (out of 6 total).
  
Model setup
: the probability $p_i$ of individual damage to each O-ring means
$$
Y_i \overset{indep}\sim \text{Bin}(n,p_i)
$$ with $g(p_i)=\beta_0 + \beta_1 x_i$, and here $n=6$.

Here is a plot of the data:

```{r}
#| label: r-oring-data-plot
#| echo: true
p1<- ggplot(orings, aes(x=temp, y=damage/6)) + 
     geom_point()+ xlim (c(25,85)) + ylim(c(0,1)) + 
     xlab ("Temperature (F)") + ylab("Probability of damage")
```

```{r}
#| label: fig-r-oring-data-plot-show
#| fig-cap: "Scatterplot of the 18 datapoints, of temperature(F) against Proportion of rings damaged."
#| fig-alt: "Scatterplot of raw data for the oring data (18 points). Showing 80% failure at around 52 degrees F, then values around 16% or 0% for most values above 55 degrees. More zeros at higher temps."
#| echo: false
#| 
#p2 <- p1 + theme(panel.background = element_rect(fill = "transparent", colour = NA),
#              #plot.background = element_rect(fill = "transparent", colour = NA),
               #panel.border = element_rect(fill = NA, colour = "black"))
p1
```

:::{.question}

## Question

Fit a binomial regression model to the data, trying out the logit, probit and complementary log-log options for the link function. 

:::
<!-- end of Task -->


::::{.answer}

Logit link:

```{r, results= 'hide'}
lmod <- glm(cbind(damage, 6-damage) ~ temp, family=binomial, data=orings) 
summary(lmod)
```

Probit link:

```{r, results='hide'}

pmod <- glm(cbind(damage, 6-damage) ~ temp, family=binomial(link="probit"), 
            data=orings)
summary(pmod)
```

Complementary log-log link:

```{r, results= 'hide'}
cmod <- glm(cbind(damage, 6-damage) ~ temp, family=binomial(link="cloglog"), data=orings) 
summary(cmod)
```

::::
<!-- end of Answer -->



:::{.question}

## Question

Superimpose the fitted probabilities from each of the three models on the above plot.

:::
<!-- end of Task -->


::::{.answer}

Here is some code for plotting the three fits.

```{r}
#| label: fig-r-oring-temp-prob-logits
#| fig-cap: "Fitted prediction curves for the three link functions, for the orings data."
#| fig-alt: "Temperature (x) against Probability of damage (y), raw data and predictions from our three link functions. Inverted S-shaped curves, from 1 down to 0 for temperatures from 30F to 80F. All similar, none fit the plotted raw data points very well."
#| fig-pos: "H" # Needed for echo: false to fix latex figure env bug

pred1 <- predict(lmod, newdata=data.frame(temp=seq(25,85,le=23)), type="response")
pred2 <- predict(pmod, newdata=data.frame(temp=seq(25,85,le=23)), type="response")
pred3 <- predict(cmod, newdata=data.frame(temp=seq(25,85,le=23)), type="response")
pred <- data.frame(logit = pred1, probit= pred2, cloglog=pred3, px = seq(25,85,le=23),orings)
p1.1 <- ggplot(pred, aes(x=orings$temp, y= orings$damage/6)) +  
        geom_point(size = 1)+ xlim (c(25,85)) + ylim(c(0,1)) + 
        xlab ("Temperature (F)") + ylab("Probability of damage") +
        geom_line(aes(x = px, y = logit, color = "Logit", linetype = "solid")) +
        geom_line(aes(x = px, y = probit, color = "Probit", linetype = "dashed"))+
        geom_line(aes(x = px, y = cloglog, color = "Complementary log-log", linetype = "dotted")) +
        guides(colour = guide_legend("Link function"), linetype = "none")
p1.1

```

::::
<!-- end of Answer -->

:::{.question}

## Question

Calculate a point estimate of the probability of damage to the O-rings when the temperature is 31 degrees Fahrenheit using each of the three models. 

:::
<!-- end of Task -->


::::{.answer}

We can obtain the predicted probabilities using the model equation:

```{r}
exp(11.6630-0.2162*31)/(1+exp(11.6630-0.2162*31))
```

We can get the same answer using the `predict()` function as follows: 

```{r}
predict(lmod, newdata=data.frame(temp=31), type="response")
```

Similarly, we can obtain the prediction for the probit model using the cumulative distribution function of a normal distribution:

```{r}
pnorm(5.5915-0.1058*31) 
```

or by using the `predict()` function:

```{r}
predict(pmod, newdata=data.frame(temp=31), type="response")
```

Finally for the complementary log-log model the predicted probability is

```{r}
predict(cmod, newdata=data.frame(temp=31), type="response")
```

The predicted probability of damage is very high for all models.

::::
<!-- end of Answer -->



## From Count Models

```{r}
#| label: r-cancer-load-data
cancer <- read.table('https://github.com/UofGAnalyticsData/APM/raw/refs/heads/main/cancer.txt',
                     header=TRUE)
head(cancer)
```

```{r}
#| label: r-cancer-glm-allvars
epid1 <- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing+offset(log(E_all)), family = poisson, data = cancer)
summary(epid1)
```

:::{.question}

## Question

For the epidemiology cancer data. Interpret the `smoke` coefficient output from the `glm()` model, in the same was we just did for `pm10`.

:::
<!-- end of Task -->

::::{.answer}

We can either take $\exp(0.00335159)=1.00336$ and interpret it as the rate ratio associated with one unit increase in the percentage of people who smoke or we can choose another percentage, *e.g.* 10\%, so that we interpret $\exp(0.00335159*10)=1.034$ as the rate ratio associated with an increase of 10 units in the percentage of people who smoke, or we can go with the standard deviation of `smoke`

```{r}
#| label: r-cancer-sd-smoke
sd(cancer$smoke)
```

which in this case turns out to be quite similar. A point estimate and an approximate confidence interval for the rate ratio associated with one standard deviation increase in the percentage of people who smoke can be obtained as follows:

```{r}
#| label: r-cancer-rate-ratio-smoke
exp(0.00335159*sd(cancer$smoke)) # point estimate

exp((0.00335159-1.96*0.0009462959)*sd(cancer$smoke)) # approx 95% CI lower limit

exp((0.00335159+1.96*0.0009462959)*sd(cancer$smoke)) # approx 95% CI upper limit
```

::::
<!-- end of Answer -->

:::{.question}

## Question

Fit a [Poisson regression model]{.term} to the Galapagos data after log-transforming the explanatory variables. Does this model fit the data better than the original [Poisson model]{.term}? Are all the explanatory variables significant? 

:::
<!-- end of Task -->


::::{.answer}

We first take log of every variable with the exception of distance to Santa Cruz for which we have to take log of `Scruz` plus a small value because Santa Cruz has distance to Santa Cruz=0.

```{r}
#| label: r-gala-glm-logs-allvars
gal4<- glm(Species ~ log(Area)+ log(Elevation) + log(Nearest) + log(Scruz+0.1) + log(Adjacent),family=poisson,data = gala)
summary(gal4)
```
In the [Poisson model]{.term} there is a substantial reduction in [deviance]{.term} when using the log-transformed variables. Also log(`Elevation)` does not appear to be significant. We can drop the term for `Elevation` from the model:

```{r}
#| label: r-gala-glm-logs-reducedvars
gal5 <- glm(Species ~ log(Area) + log(Nearest) + log(Scruz+0.1) + log(Adjacent),family=poisson, data = gala)
summary(gal5)
```

Since there are still signs of [overdispersion]{.term}, we will choose to estimate the [dispersion parameter]{.term} and use it in a [quasi-Poisson model]{.term}:

```{r}
#| label: r-gala-glm-logs-quasisummary
#dispersion parameter
dp <- sum(residuals(gal5,type="pearson")^2)/gal5$df.res

summary(gal5, dispersion=dp)

```
It looks like more terms can be dropped. When dropping terms, make sure you don't drop more than one at a time.

```{r}
#| label: r-gala-glm-logs-drop1
drop1(gal5, test="F")
```
It looks like distance to Santa Cruz can be dropped.

Repeating the process we see that distance to nearest neighbour can also be dropped and the final model is one with terms for the logarithm of the area of the island and the logarithm of the area of the adjacent island.

::::
<!-- end of Answer -->



:::{.question}

## Question

In the epidemiology case study (start of "Models for count data" section), the [residual deviance]{.term} of the fitted model is quite high compared to the degrees of freedom. Could this be due to [overdispersion]{.term}? Fit a [quasi-Poisson model]{.term} and a [negative binomial model]{.term} and explore diagnostic plots to check if one of these might be more appropriate.

:::
<!-- end of Task -->


::::{.answer}

Start by fitting a [quasi-Poisson model]{.term} following the analysis steps for the Galapagos data:

```{r}
#| label: r-cancer-quasi-summary-and-phi

epid2 <- glm(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing+offset(log(E_all)), family=quasipoisson(link = "log"), data = cancer)
summary(epid2)

# Calculate the dispersion parameter:
pearson <- residuals(epid1, type = "pearson")
sum(pearson^2)/epid1$df.residual
```

```{r}
#| label: fig-cancer-poisson-vs-quasi
#| fig-cap: "Plots of predicted counts against Standardised Pearson Residuals, for regular Poisson likelihood and modified Quasi-likelihood."
#| fig-alt: "Around 10 points lie outside (-3,3) in the plot of Standardised Pearson Residuals for the regular likelihood, but for the Quasi-likelihood we now don't see such outliers, the distribution is far more believable Normal(0,1)."
#| echo: false
#| fig-pos: "H"

# Compute predictions and standardized residuals
pred1 <- predict(epid1, type = "response")
resid1 <- rstandard(epid1, type = "pearson")

pred2 <- predict(epid2, type = "response")
resid2 <- rstandard(epid2, type = "pearson")

# Prepare data for plot and order of plots
plotdata <- rbind(
  data.frame(pred = pred1, resid = resid1, Model = "Regular likelihood"),
  data.frame(pred = pred2, resid = resid2, Model = "Quasi-likelihood")
)

plotdata$Model <- factor(plotdata$Model, levels = c("Regular likelihood", "Quasi-likelihood"))

ggplot(plotdata, aes(x = pred, y = resid)) +
  geom_point(shape = 1, alpha = 0.7, size = 3) +
  geom_hline(yintercept = c(-3, -2, 0, 2, 3), linetype = "dotted", color = "red") +
  facet_wrap(~Model) +
  xlab("Predicted count") +
  ylab("Standardized Pearson residuals") +
  ylim(-5, 5) + theme(strip.text = element_text(size = 14))


```

We see that for the cancer data, just like for the Galapagos data, all of the [residuals]{.term} are contained within $\pm 3$ in the [quasi-Poisson model]{.term}, while quite a few are outside this range for the original [Poisson model]{.term}.

Next we fit a [negative binomial model]{.term}:

```{r, fig.height=4}
#| label: r-cancer-nb-logs
library(MASS)
epid3 <- glm.nb(Y_all ~ pm10 + smoke + ethnic + log.price + easting + northing + offset(log(E_all)), data = cancer)
summary(epid3)
```

To decide between the [quasi-Poisson]{.term} and the [negative binomial model]{.term}, we plot $(y_i-\hat{\mu}_i)^2$ versus $\hat{\mu}_i$ and compare the linear and quadratic fit to see which one of them fits better.

<!--
```{r}
#| label: fig-r-XXX7
#| fig-cap: "XXX"
#| fig-alt: "XXX"
# fig.height=4, fig.width=5

# Plot of squared residuals v predicted
res.sq <- residuals(epid1, type = "response")^2
set1 <- data.frame(res.sq, mu.hat = epid1$fitted.values)

fit.lin <- lm(formula = res.sq ~ mu.hat, data = set1)
fit.quad <- lm(formula = res.sq ~ mu.hat + I(mu.hat^2), data = set1)
summary(fit.quad)

plot(set1$mu.hat, y = set1$res.sq, xlab = "Predicted count",
     ylab = "Squared Residual")
curve(expr = predict(fit.lin, newdata = data.frame(mu.hat = x), type = "response"),
      col = "blue", add = TRUE, lty = "solid")
curve(expr = predict(fit.quad, newdata = data.frame(mu.hat = x), type = "response"),
      col = "red", add = TRUE, lty = "dashed")
legend("topleft", legend = c("Linear", "Quadratic"), col = c("blue","red"),
    lty = c("solid", "dashed"), bty = "n")
```
-->

```{r}
#| label: fig-r-cancer-nb-versus-quasi-plot
#| fig-cap: "Scatterplot of squared raw residuals against predicted counts. Overlaid are lines of best fit, linear and quadratic."
#| fig-alt: "Both linear and quadratic lines of best fit are very similar. Points show a general upward trend, but at best a weak linear correlation. No good reason to prefer the more complex quadratic."
#| echo: false
#| fig-pos: "H"

res.sq <- residuals(epid1, type = "response")^2
set1 <- data.frame(res.sq, mu.hat = epid1$fitted.values)

fit.lin <- lm(formula = res.sq ~ mu.hat, data = set1)
fit.quad <- lm(formula = res.sq ~ mu.hat + I(mu.hat^2), data = set1)

# Choose x-resolution and generate lines
mu.seq <- seq(min(set1$mu.hat), max(set1$mu.hat), length.out = 12)
pred.lin <- predict(fit.lin, newdata = data.frame(mu.hat = mu.seq))
pred.quad <- predict(fit.quad, newdata = data.frame(mu.hat = mu.seq))

pred.df <- data.frame(
  mu.hat = rep(mu.seq, 2),
  res.sq = c(pred.lin, pred.quad),
  model = factor(rep(c("Linear", "Quadratic"), each = length(mu.seq)))
)

ggplot(set1, aes(x = mu.hat, y = res.sq)) +
  geom_point(shape = 1) + 
  geom_line(data = pred.df, aes(x = mu.hat, y = res.sq, color = model, linetype = model)) +
  scale_linetype_manual(values = c("Linear" = "solid", "Quadratic" = "dashed")) +
  scale_color_manual(values = c("Linear" = "blue", "Quadratic" = "red")) +
  xlab("Predicted count") + ylab("Squared Residual") +
  labs(color = NULL, linetype = NULL) +  # remove legend title
  theme(
    legend.position = c(0.05, 0.95),  # top-left inside plot
    legend.justification = c(0, 1),   
    legend.key.width = unit(2, "cm"),
    legend.background = element_rect(  # box around legend
      color = "black",
      size = 0.2
    )
  )
```


The straight line fit (blue line) would indicate that the [quasi-Poisson model]{.term} should be preferred, and the quadratic fit (red line) would suggest using a [negative binomial model]{.term}.

The quadratic coefficient in the above model has a $p$-value of 0.411 -- not significant. We don't have any evidence that the [negative binomial model]{.term} is better and we can probably go with either one.
::::
<!-- end of Answer -->

## INSERT SOME QUESTION ABOUT ZIP OR SEPARATION**

## A case study: Olympic medal predictions

:::{.Example}

### Predicting the total number of medals in the 2012 Olympics

We wish to model the number of medals won by each country in the London Olympics in 2012 as a function of the countryâ€™s population and GDP per capita. The dataset `OlympicMedals2012.csv` contains this information for all the countries that won at least one medal in the 2012 Games. A ranking of countries based on the total medals won in the Olympics can be found at https://en.wikipedia.org/wiki/2012_Summer_Olympics_medal_table.
We start by creating a variable for GDP per capita in 1000 US dollars and looking at some plots of the data.

```{r}
#| label: r-olympics-load-data
library(gridExtra)
olympics0 <- read.csv('https://github.com/UofGAnalyticsData/APM/raw/refs/heads/main/OlympicMedals2012.csv')
olympics <- data.frame(country = olympics0$Country, medals = olympics0$Medals,
                        population = olympics0$Population, 
                        gold = olympics0$Gold.Medal,
                        GDP = olympics0$GDP..US.Billion)
olympics$GDPpercapita <- olympics$GDP * 10^6 / olympics$population
head(olympics)
```
The wide range of values for both population and GDP per capita suggests log-transforming both explanatory variables. From the plots we see a positive association between log(population) and log(medals) and also between log(GDP per capita) and log(medals). 

```{r}
#| label: fig-r-XXX8
#| fig-cap: "XXX"
#| fig-alt: "XXX"
#| fig-height: 3
p1 <- ggplot(olympics, aes(x=log(population), y=log(medals))) +
       geom_point()
p2 <- ggplot(olympics, aes(x=log(GDPpercapita), y=log(medals))) +
       geom_point()

grid.arrange(p1, p2, nrow=1)
```

We start the analysis by fitting a [Poisson regression model]{.term}.

```{r}
#| label: r-olympics-glm-logs
ol1 <- glm(medals ~ log(population) + log(GDPpercapita), 
              family = poisson, data = olympics)
summary(ol1)
```

Large population and high GDP per capita appear to contribute to Olympic success as indicated by the positive coefficients of both terms in the [Poisson model]{.term}. 
:::
<!-- end of Example -->


<!-- old redacted content removed -->

:::{.question}

## Question

The fit of the [Poisson model]{.term} is not particularly good (just look at the [deviance]{.term} of 547.5 on 82 degrees of freedom). Supposing that this is due to [overdispersion]{.term}, fit a [quasi-Poisson model]{.term} to adjust for it.

:::
<!-- end of Task -->


::::{.answer}

Estimate the [dispersion parameter]{.term}: 

```{r}
#| label: r-olympics-glm-dispersion
X2 <- sum(resid(ol1, type = "pearson")^2)
X2
dp <- X2/ol1$df.res
dp
```

Refit the model using the [dispersion parameter]{.term}:
```{r}
#| label: r-olympics-glm-disp-summary-drop1
summary(ol1, dispersion= dp)

drop1(ol1, type="F")
```

The parameter estimates (and fitted values) are the same, and although the standard errors change, both predictors are still highly significant.
::::
<!-- end of Answer -->





:::{.question}

## Question

Fit a [negative binomial model]{.term} to the Olympics data.

:::
<!-- end of Task -->


::::{.answer}

A [negative binomial model]{.term} can be fit using

```{r}
#| label: r-olympics-nb-logs
library(MASS)
ol2 <- glm.nb(medals ~ log(population) + log(GDPpercapita), data=olympics)
summary(ol2)
```

::::
<!-- end of Answer -->



:::{.question}

## Question

Examine the [residuals]{.term} from the [negative binomial model]{.term} and comment on any unusual observations.

:::
<!-- end of Task -->


::::{.answer}


Pearson residual plots for this model:
```{r}
#| label: fig-r-XXX9
#| fig-cap: "XXX"
#| fig-alt: "XXX"
res <- resid(ol2, type = "pearson")
d2 <- data.frame(res=resid(ol2, type = "pearson"))

p3 <- ggplot(d2, aes(sample=res)) + geom_point(stat="qq") +
    xlab("Theoretical quantiles") + ylab("Sample quantiles")

p4 <- ggplot(d2, aes(x=predict(ol2, type="link"), y=res)) +
        geom_point() + geom_hline(yintercept = 0) +
        xlab("Linear predictor") + ylab("Pearson residuals")

p5 <- ggplot(d2, aes(x=log(olympics$GDPpercapita), y=res)) +
        geom_point() + xlab("log(GDP per capita)") +
        ylab("Pearson residuals")

p6 <- ggplot(d2, aes(x=log(olympics$population), y=res)) +
        geom_point() + xlab("log(population)") +
        ylab("Pearson residuals")

grid.arrange(p3, p4, p5, p6, nrow = 2)
```

The normal probability plot shows some large positive [residuals]{.term}. No patterns are seen in the plot of [residuals]{.term} against the linear predictor, and there is no obvious nonlinear pattern in the plots of [residuals]{.term} against the explanatory variables. 

We can also look at the cases where the model does not fit the data better (these correspond to large positive or negative [residuals]{.term}).
```{r}
#| label: r-olympics-nb-logs-pearson
pres.n2 <- resid(ol2, type = "pearson")
expmedals2 <- round(fitted(ol2)[abs(pres.n2) > 1], 2)
cbind(olympics[abs(pres.n2) > 1, 1:2], expmedals2)
```

Here we see that the model does a poor job of predicting strong Olympic performances by countries such as the USA, UK and China, and not so strong performances by countries such as India and Indonesia.
 
::::
<!-- end of Answer -->




:::{.question}

## Question

Do you think that either the [Poisson]{.term} or the [negative binomial model]{.term} with log(GDP) and log(population) as predictors would do a good job predicting the total number of medals won by countries in 2012? Explain.

:::
<!-- end of Task -->


::::{.answer}

Clearly there is more to winning Olympic medals than just the country's population and GDP per capita. The large [residuals]{.term} from the [negative binomial model]{.term} show that this is the case. Adjusting for [overdispersion]{.term} does not improve predictive performance in general, and may not be as useful as including additional predictors in the model.
::::
<!-- end of Answer -->

:::{.question}

## Question

In the Olympic medals example, suppose we also had data on the countries that participated in the 2012 Olympics but did not win any medals. What type of model would you consider for these data and why?

:::
<!-- end of Task -->


::::{.answer}

A [zero-inflated]{.term} Poisson or [negative binomial model]{.term} might be appropriate for simultaneously considering

* the probability of a country winning a medal in the 2012 Olympics (using a logit model), and

* the number of medals won (using a [Poisson]{.term} or [negative binomial model]{.term}).

In practice such a model could be fit using function `zero.infl()` from `library(pscl)`.

::::
<!-- end of Answer -->

## Simpson's paradox

Simpson's paradox is a phenomenon in which associations get reversed when we look at aggregates. A very simple example of this is the following:

Suppose that I get better grades than you in easy courses and I also get better grades than you in hard courses, and yet your GPA is higher than mine. How can this be? 

I take lots of hard courses where I get mostly Cs and you get mostly Ds, and you take lots of easy courses where you get mostly Bs and I get mostly As. Even though I do better than you when we control for the difficulty of the course, your overall GPA will be higher.

Here is another example with a contingency table.


:::{.Example}

### Death penalty in the US


This dataset comes from *Categorical Data Analysis* by Alan Agresti. The $2 \times 2 \times 2$ table shows homicide cases in Florida over the period 1976-77. The defendant's race and victim's race, each having categories white or black, and whether there was a death penalty verdict (yes/no), was recorded. 


+-------------------+---------------+-----+-----+-------------+
| Defendant's Race  | Victim's Race | Yes | No  | Percentage  |
+===================+===============+=====+=====+=============+
|                   | White         | 19  | 132 | 12.6        |
| White             +---------------+-----+-----+-------------+
|                   | Black         | 0   | 9   | 0.0         |
+-------------------+---------------+-----+-----+-------------+
|                   | White         | 11  | 52  | 17.5        |
| Black             +---------------+-----+-----+-------------+
|                   | Black         | 6   | 97  | 5.8         |
+-------------------+---------------+-----+-----+-------------+

: Full original data of death penalty verdicts {#tbl-racedeath-alltable-grid tbl-colwidths="[15,15,15,15,15]" .striped .hover}

The following is the marginal table obtained by summing the cell counts over the levels of victim's race.

| Defendant's Race | Death Penalty: Yes | Death Penalty: No | Total   | Percentage (Yes) |
|:-----------------|:------------------:|:-----------------:|:-------:|:----------------:|
| White            | 19                 | 141               | 160     | 11.9             |
| Black            | 17                 | 149               | 166     | 10.2             |
| Total            | 36                 | 290               | 326     |                  |

: Marginal summaries, ignoring victim's race {#tbl-deathrace-marginal tbl-colwidths="[20,20,20,10,15]" .striped .hover}

About 12\% of white defendants and about 10\% of black defendants received the death penalty. Ignoring the victim's race, the percentage of "yes" death penalty verdicts was lower for blacks than for whites.

However taking the victim's race into account, things look completely different: When the victim was white, the death penalty was imposed about 5 percentage points more often for black defendants than for white defendants. When the victim was black, the death penalty was imposed over 5 percentage points more often for black defendants than for white defendants. Controlling for the victim's race, the percentage of "yes" death penalty verdicts was higher for blacks than for whites.

The phenomenon in which a pair of variables have marginal association of different direction from their partial associations is called *Simpson's paradox*. In the death penalty example, it arises because whites tended to kill whites, and killing a white person was more likely to result in the death penalty.
:::
<!-- end of Example -->

:::{.question}

## Question

Fit a suitable loglinear model to the death penalty data and interpret it in terms of the associations between the variables.

:::
<!-- end of Task -->

::::{.answer}


Let $P$ stand for death penalty, $V$ for victim's race and $D$ for defendant's race. Create a data.frame with the data:
```{r}
#| label: r-racedeath-simpsons-table
deathpenalty <- data.frame (D=rep(c("white","white","black","black"),2),
                            V=rep(c("white","black"),4),
                            P=c(rep("yes",4),rep("no",4)),
                            freq = c(19,0,11,6,132,9,52,97))

xtabs(freq ~ D + V+ P, data=deathpenalty)
```

Start from fitting the full independence model:

```{r}
#| label: r-racedeath-glm-additive
glm(freq ~ D + V+ P, family=poisson, data=deathpenalty)
```
and try all models up to this fully saturated model:

```{r}
#| label: r-racedeath-glm-interactions
glm(freq ~ D*V*P, family=poisson, data=deathpenalty)
```

| Terms in model | Deviance  | Degrees of Freedom |
|:---------------|----------:|-------------------:|
| [D,V,P]        | 137.93    | 4                  |
| [D,VP]         | 131.68    | 3                  |
| [V,DP]         | 137.71    | 3                  |
| [P,DV]         | 8.13      | 3                  |
| [DP,VP]        | 131.46    | 2                  |
| [DP,DV]        | 7.91      | 2                  |
| [VP,DV]        | 1.88      | 2                  |
| [DP,VP,DV]     | 0.70      | 1                  |
| [DVP]          | 0.00      | 0                  |

: List of deviances and degrees of freedom for all possible models {#tbl-racedeath-deviances-allmodels .striped .hover}

Notice that any model that does not include the term `D*V` has a large [deviance]{.term}. This suggests an important association between the defendant's race and the victim's race. Other than the [saturated model]{.term}, the two models that appear to fit the data well when we compare the [deviance]{.term} with a chi-squared distribution with the corresponding [degrees of freedom]{.term}, are $[VP,DV]$  and $[DP,VP,DV]$. The simpler model says that the death penalty verdict is independent of the defendant's race, given the victim's race. In the model with all two-way interactions, all pairs of variables are conditionally dependent.
::::
<!-- end of Answer -->

